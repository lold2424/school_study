# -*- coding: utf-8 -*-
"""2019250033_윤혜진_3차과제

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nIAPSP6NgBTlpZc1nrjvKUzT6QP4cl7R

# (실습) 머신러닝 프로젝트 처음부터 끝까지 2부

**참고**

[(구글코랩) 머신러닝 프로젝트 처음부터 끝까지](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_end2end_ml_project.ipynb)
의 소스코드를 먼저 공부하세요.
"""

import sys
assert sys.version_info >= (3, 7)

import sklearn
assert sklearn.__version__ >= "1.0.1"

import numpy as np
np.random.seed(42)

from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)
from pandas.plotting import scatter_matrix

"""### **문제 1**

아래의 세 가지 변환을 전처리 과정에 추가하면 훈련된 모델의 성능이 어떻게 얼마나 달라지는지 이전 모델과 비교하라.

* 변환 1

중간 소득과 중간 주택 가격 사이의 상관관계 그래프에서 확인할 수 있는 수평선에 위치한 데이터를 삭제한다.

* 변환 2

강의노트에서 소개된 전처리를 통해 최종적으로 생성된 24개의 특성 중에서
중간 주택 가격과의 상관계수의 절댓값이 0.2 보다 작은 특성을 삭제한다.

힌트: 

1. 아래 사이트를 참조할 것.
    - [캐글(kaggle) 참고 페이지 1](https://www.kaggle.com/khushboon/california-housing-price-prediction)
    - [캐글(kaggle) 참고 페이지 2](https://www.kaggle.com/suprabhatsk/california-housing-prices-prediction)
    - [캐글(kaggle) 참고 페이지 3](https://www.kaggle.com/subashdump/california-housing-price-prediction)
1. 상관관계 도표에서 수평선 위치 찾기: [value_count() 메서드 활용](https://www.w3resource.com/pandas/series/series-value_counts.php)
"""

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")                      # 파일 저장 경로. 파일명 함께 지정.
    if not tarball_path.is_file():                                   # 아직 다운로드하지 않은 경우
        Path("datasets").mkdir(parents=True, exist_ok=True)          # 폴더 생성
        url = "https://github.com/ageron/data/raw/main/housing.tgz"  # 파일 다운로드
        urllib.request.urlretrieve(url, tarball_path)
        with tarfile.open(tarball_path) as housing_tarball:          # tgz 파일 압축 풀기
            housing_tarball.extractall(path="datasets")
    return pd.read_csv(Path("datasets/housing/housing.csv"))         # 압축 풀린 csv 파일 불러오기

housing = load_housing_data()

#소득 구간을 새로운 범주형 특성으로
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])

#소득을 기준으로 계층 샘플링
strat_train_set, strat_test_set = train_test_split(housing, 
                                                   test_size=0.2, 
                                                   stratify=housing["income_cat"], 
                                                   random_state=42)
# 소득 구간별 비율 계산
def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)

# 비교 대상: 무작위 샘플링
train_set, test_set = train_test_split(housing, 
                                       test_size=0.2, 
                                       random_state=42)

for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)

housing.plot(kind="scatter", 
             x="median_income", 
             y="median_house_value",
             alpha=0.1, 
             grid=True)
plt.show()

"""### 변환 1

표로 나타냈을때 최댓값이 500,001으로 고정된것을 보아 500,001 이상의 값은 전부 500,001로 처리한것을 알 수 있다.
"""

housing_sorted_by_values = housing.sort_values(by=["median_income","median_house_value"] ,ascending=False)
housing_sorted_by_values.loc[:, ["median_income","median_house_value"]]

housing.value_counts("median_house_value").iloc[:50]

"""위 표를 보면 500,001이 965개로 가장 많았고 그 아래로도 여러개의 값을 지닌 데이터가 확인된다.

이는 수평선을 만드는 원인이기 때문에 제거를 진행할건데, 그전에 너무 많은 데이터를 삭제하면 안되니 30까지의 데이터를 가진 값을 제거하겠다.
"""

rmData = housing.value_counts("median_house_value").iloc[:24]
#rmData.index
for i in rmData.index:
  housing = housing[housing.median_house_value != i]

"""제거를 완료했으니 다시 시각화를 통해 확인해보자"""

housing.plot(kind="scatter", 
             x="median_income", 
             y="median_house_value",
             alpha=0.1, 
             grid=True)
plt.show()

"""### 변환 2

#### 전처리로 특성 24개 생성

우선 샘플링한 데이터를 지워준다.
"""

# 입력 데이터셋 지정
#housing = strat_train_set.drop("median_house_value", axis=1)
# 타깃 데이터셋 지정
#housing_labels = strat_train_set["median_house_value"].copy()
housing_labels = housing["median_house_value"].copy()
housing = housing.drop("income_cat", axis=1)
housing = housing.drop("median_house_value", axis=1)

null_rows_idx = housing.isnull().any(axis=1)
null_rows_idx

housing.loc[null_rows_idx].shape #187개의 결측치

# 중앙값으로 결정 
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")

housing_num = housing.select_dtypes(include=[np.number])

imputer.fit(housing_num)

imputer.statistics_

#중앙값으로 설정
X = imputer.transform(housing_num)
X

housing_tr = pd.DataFrame(X, 
                          columns=housing_num.columns,
                          index=housing_num.index)
# total_bedrooms 특성에 결측치를 포함했던 5개 샘플
housing_tr.loc[null_rows_idx].head()

housing_cat = housing[["ocean_proximity"]]

#범주 수 만큼 새로운 특성 추가
from sklearn.preprocessing import OneHotEncoder

cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)

cat_encoder = OneHotEncoder(sparse_output=False)
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot

cat_encoder.categories_ #변환에 사용된 범주들

cat_encoder.feature_names_in_ # 변환된 특성의 이름

cat_encoder.get_feature_names_out() #새로 생성된 특성의 이름

df_output = pd.DataFrame(housing_cat_1hot,
                         columns=cat_encoder.get_feature_names_out(),
                         index=housing_cat.index)

df_output

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.pipeline import Pipeline
from sklearn.pipeline import make_pipeline
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_selector
from sklearn.compose import make_column_transformer

class ClusterSimilarity(BaseEstimator, TransformerMixin):
    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):
        self.n_clusters = n_clusters
        self.gamma = gamma
        self.random_state = random_state

    # 군집화
    def fit(self, X, y=None, sample_weight=None):
        self.kmeans_ = KMeans(self.n_clusters, random_state=self.random_state)
        self.kmeans_.fit(X, sample_weight=sample_weight)
        return self  # fit() 함수의 반환값은 언제나 self!

    # fit() 이 찾아낸 군집별 유사도를 새로운 특성으로 추가
    def transform(self, X):
        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)
    
    # 새롭게 생성된 특성 이름
    def get_feature_names_out(self, names=None):
        return [f"Cluster {i} similarity" for i in range(self.n_clusters)]

# 비율 변환기
def column_ratio(X):
    return X[:, [0]] / X[:, [1]] # 1번 특성에 대한 0번 특성의 비율율

def ratio_name(function_transformer, feature_names_in):
    return ["ratio"]  # 새로 생성되는 특성 이름에 추가

def ratio_pipeline():
    return make_pipeline(
        SimpleImputer(strategy="median"),
        FunctionTransformer(column_ratio, feature_names_out=ratio_name),
        StandardScaler())

#로그 변환기
log_pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    FunctionTransformer(np.log, feature_names_out="one-to-one"),
    StandardScaler())

#군집 변환기
cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)

#결측치 처리와 표준화 전처리
default_num_pipeline = make_pipeline(SimpleImputer(strategy="median"),
                                     StandardScaler())

# 범주형 특성 파이프라인
cat_pipeline = make_pipeline(
    SimpleImputer(strategy="most_frequent"),
    OneHotEncoder(handle_unknown="ignore"))

# 정제와 전처리 과정 전체를 아우루는 변환 파이프라인
preprocessing = ColumnTransformer([
        ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"]),    # 방당 침실 수
        ("rooms_per_house", ratio_pipeline(), ["total_rooms", "households"]), # 가구당 침실 수
        ("people_per_house", ratio_pipeline(), ["population", "households"]), # 가구당 인원
        ("log", log_pipeline, ["total_bedrooms", "total_rooms", "population", # 로그 변환
                               "households", "median_income"]),
        ("geo", cluster_simil, ["latitude", "longitude"]),                    # 구역별 군집 정보
        ("cat", cat_pipeline, make_column_selector(dtype_include=object)),    # 범주형 특성 전처리
    ],
    remainder=default_num_pipeline)                                           # 중간 주택 년수(housing_median_age) 대상

housing_prepared = preprocessing.fit_transform(housing)
housing_prepared.shape

#특성 이름
preprocessing.get_feature_names_out()

#변환된 입력 데이터셋
housing_prepared

housing_prepared_df = pd.DataFrame(housing_prepared,
                                   columns=preprocessing.get_feature_names_out(),
                                   index=housing.index)

housing_prepared_df.head()

"""#### median_house_value와 상관관계

중간 주택 가격과 다른 특성간의 상관계수를 알기 위해 median_house_value 추가해준다.
"""

housing_prepared_df["median_house_value"] = housing_labels
housing_prepared_df

corr_matrix = housing_prepared_df.corr()
corr_matrix = corr_matrix["median_house_value"]
corr_matrix

"""위 24개의 특성과 `median_house_value`와 상관계수의 절댓값이 0.2보다 작다면 제거를 진행한다.

상관계수를 구하기전에 주의해야하는점이 있다.

상관계수는 연속적인 변수 간 관계를 측정하기 때문에, 범주형 데이터에서는 다른 지표를 사용해야 한다.
"""

rmList = []
i = 0

while i < len(housing_prepared_df.columns):
  if abs(corr_matrix[i]) < 0.2:
    rmList.append(housing_prepared_df.columns[i])
  i += 1

housing_prepared_df = housing_prepared_df.drop(rmList, axis = 1)
housing_prepared_df = housing_prepared_df.drop("median_house_value", axis = 1)
housing_prepared_df

"""### 훈련된 모델 성능 비교

#### **선형 회귀 모델**
"""

from sklearn.linear_model import LinearRegression

lin_reg = make_pipeline(preprocessing, LinearRegression())
lin_reg.fit(housing, housing_labels)

housing_predictions = lin_reg.predict(housing)
housing_predictions[:5].round(-2)  # -2 = 10의 자리에서 반올림하기기

housing_labels.iloc[:5].values

error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1
print(", ".join([f"{100 * ratio:.1f}%" for ratio in error_ratios]))

from sklearn.metrics import mean_squared_error

lin_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
lin_rmse

"""**예측값의 rmse = 57301**
- 변환이 적용되기 이전 모델 : 68687
- 이전 모델에 비해 오차가 줄어든게 확인된다.

#### **결정트리 회귀 모델**
"""

from sklearn.tree import DecisionTreeRegressor

tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))
tree_reg.fit(housing, housing_labels)

housing_predictions = tree_reg.predict(housing)
tree_rmse = mean_squared_error(housing_labels, housing_predictions,
                              squared=False)
tree_rmse

"""**RMSE = 0**
- 변환이 적용되기 이전 모델 = 0
- 이전 모델과 동일하게 과대 적합인것이 확인된다.

#### **교차 검증**

결정트리 모델에 대한 교차 검증
"""

from sklearn.model_selection import cross_val_score

tree_rmses = -cross_val_score(tree_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", 
                              cv=10)

tree_rmses

pd.Series(tree_rmses).describe()

"""선형트리 교차 검증"""

lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,
                              scoring="neg_root_mean_squared_error", cv=10)
pd.Series(lin_rmses).describe()

"""랜덤 포레스트 회귀 모델과 교차검증"""

from sklearn.ensemble import RandomForestRegressor

forest_reg = make_pipeline(preprocessing,
                           RandomForestRegressor(n_estimators=100, random_state=42))

forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,
                                scoring="neg_root_mean_squared_error", cv=10)

pd.Series(forest_rmses).describe()

forest_reg.fit(housing, housing_labels)
housing_predictions = forest_reg.predict(housing)

forest_rmse = mean_squared_error(housing_labels, housing_predictions,
                                 squared=False)
forest_rmse

"""### 그리드 탐색"""

from sklearn.model_selection import GridSearchCV

full_pipeline = Pipeline([
    ("preprocessing", preprocessing),
    ("random_forest", RandomForestRegressor(random_state=42)),
])

# 하아퍼파라미터 조합 후보
param_grid = [
    {'preprocessing__geo__n_clusters': [5, 8, 10],  # ClusterSimilarity 클래스 하이퍼파라미터: 군집 수
     'random_forest__max_features': [4, 6, 8]},     # 랜덤 포레스트 하이퍼파라미터
    {'preprocessing__geo__n_clusters': [10, 15],
     'random_forest__max_features': [6, 8, 10]},
]

grid_search = GridSearchCV(full_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')

grid_search.fit(housing, housing_labels)

list(full_pipeline.get_params().keys())[:10]

'preprocessing__geo__n_clusters' in full_pipeline.get_params().keys()

grid_search.best_params_

grid_search.best_estimator_

cv_res = pd.DataFrame(grid_search.cv_results_)
cv_res.head()

cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)
cv_res.head()

cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", 
                 "split0_test_score",
                 "split1_test_score", 
                 "split2_test_score", 
                 "mean_test_score"]]
cv_res.head()

score_cols = ["split0", "split1", "split2", "mean_test_rmse"]
cv_res.columns = ["n_clusters", "max_features"] + score_cols
cv_res.head()

cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)

cv_res.head()

"""### 랜덤 탐색"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'preprocessing__geo__n_clusters': randint(low=3, high=50), # ClusterSimilarity 클래스 하이퍼파라미터: 군집 수
                  'random_forest__max_features': randint(low=2, high=20)}

rnd_search = RandomizedSearchCV(
    full_pipeline, param_distributions=param_distribs, n_iter=10, cv=3,
    scoring='neg_root_mean_squared_error', random_state=42)

rnd_search.fit(housing, housing_labels)

cv_res = pd.DataFrame(rnd_search.cv_results_)

# RMSE 기준 내림차순 정렬
cv_res.sort_values(by="mean_test_score", ascending=False, inplace=True)

# 일부 특성만 대상으로 삼기
cv_res = cv_res[["param_preprocessing__geo__n_clusters",
                 "param_random_forest__max_features", "split0_test_score",
                 "split1_test_score", "split2_test_score", "mean_test_score"]]
cv_res.columns = ["n_clusters", "max_features"] + score_cols

# 점수를 양수로 변환
cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)
cv_res.head()

"""### 최적 모델 활용"""

final_model = rnd_search.best_estimator_                                 # 최적의 모델
feature_importances = final_model["random_forest"].feature_importances_  # 특성별 상대적 중요도

sorted(zip(feature_importances,
           final_model["preprocessing"].get_feature_names_out()),
           reverse=True)

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

final_predictions = final_model.predict(X_test)

final_rmse = mean_squared_error(y_test, final_predictions, squared=False)
print(final_rmse)

"""### **문제 2**

[(구글코랩) 머신러닝 프로젝트 처음부터 끝까지](https://colab.research.google.com/github/codingalzi/handson-ml3/blob/master/notebooks/code_end2end_ml_project.ipynb) 의 
맨 아래에 있는 연습문제 1번부터 6번까지를 따라하면서 내용을 정리하라.

#### 1.
다양한 하이퍼 파라미터(kernel="linear", kernel="rbf")를 사용해 `Support Vector Machine` 회귀자(sklearn.svm.SVR)를 사용했다.

SVM은 대규모 데이터 셋 스케일링에 적합하지 않아 5000개에 대해서 모델에 학습 시키고 3배 교차검증을 사용했다.
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR

param_grid = [
        {'svr__kernel': ['linear'], 'svr__C': [10., 30., 100., 300., 1000.,
                                               3000., 10000., 30000.0]},
        {'svr__kernel': ['rbf'], 'svr__C': [1.0, 3.0, 10., 30., 100., 300.,
                                            1000.0],
         'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},
    ]

svr_pipeline = Pipeline([("preprocessing", preprocessing), ("svr", SVR())])
grid_search = GridSearchCV(svr_pipeline, param_grid, cv=3,
                           scoring='neg_root_mean_squared_error')
grid_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])

"""가장 좋은 모델은 다음과 같은 값이 나왔다."""

svr_grid_search_rmse = -grid_search.best_score_
svr_grid_search_rmse

"""랜덤 포레스트 회귀 분석기 보다 안좋다는것을 알 수 있다.
- 찾아낸 최고의 하이퍼 파라미터를 확인해 보자
"""

grid_search.best_params_

"""선형 kernel이 RBF(랜덤 포레스트 회귀 분석기)보다 좋다는것을 알 수 있다.
- C: 최대 테스트 값
- C 값이 높을 수록 더 좋을 수 있기 때문에, C 값을 높여서 그리드 탐색을 다시 해야한다.

#### 2.
그리드 탐색 CV(GridSearchCV)를 랜덤 탐색 CV(RandomizedSearchCV)로 바꿔보자
"""

from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import expon, reciprocal

# see https://docs.scipy.org/doc/scipy/reference/stats.html
# for `expon()` and `reciprocal()` documentation and more probability distribution functions.

# Note: gamma is ignored when kernel is "linear"
param_distribs = {
        'svr__kernel': ['linear', 'rbf'],
        'svr__C': reciprocal(20, 200_000),
        'svr__gamma': expon(scale=1.0),
    }

rnd_search = RandomizedSearchCV(svr_pipeline,
                                param_distributions=param_distribs,
                                n_iter=50, cv=3,
                                scoring='neg_root_mean_squared_error',
                                random_state=42)
rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])

"""최적의 모델이 가지는 값은 아래와 같다."""

svr_rnd_search_rmse = -rnd_search.best_score_
svr_rnd_search_rmse

"""전보다는 나아졌으나 여전히 RBF 성능에는 못미치는게 확인된다."""

rnd_search.best_params_

"""RBF 커널에 더 좋은 하이퍼 파라미터를 찾았다.
- 같은 시간 내 랜덤 탐색은 그리드 탐색보다 더 좋은 하이퍼 파라미터를 찾아준다.

gamma에 대해 크기 1의 expon() 분포를 사용하면 랜덤 탐색이 해당 크기 값을 검색해준다.
- 표본의 약 80%가 0.1과 2.3 사이로 나온다.
"""

np.random.seed(42)

s = expon(scale=1).rvs(100_000)  # get 100,000 samples
((s > 0.105) & (s < 2.29)).sum() / 100_000

"""C에 대해 reciprocal()(역) 분포 사용했다.

즉, 랜덤 탐색 전 C의 최적 크기가 무엇인지 모르게된다.

#### 3.
preparation 파이프라인에 SelectFromModel 변환기를 추가해서 가장 중요한 속성 선택

이전에 정의된 `preparation` 파이프라인을 실행할 새로운 파이프라인 만들고 최종 회귀 분석기 앞에 랜덤 포레스트 회귀 분석기 기반의 `SelectFromModel` 변환기를 추가했다.
"""

from sklearn.feature_selection import SelectFromModel

selector_pipeline = Pipeline([
    ('preprocessing', preprocessing),
    ('selector', SelectFromModel(RandomForestRegressor(random_state=42),
                                 threshold=0.005)),  # min feature importance
    ('svr', SVR(C=rnd_search.best_params_["svr__C"],
                gamma=rnd_search.best_params_["svr__gamma"],
                kernel=rnd_search.best_params_["svr__kernel"])),
])

selector_rmses = -cross_val_score(selector_pipeline,
                                  housing.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  scoring="neg_root_mean_squared_error",
                                  cv=3)
pd.Series(selector_rmses).describe()

"""임계값이 최적이 아니라 특성 선택이 도움이 되는것 같지는 않다.

#### 4.
k-Nearest Neighbors regressor를 훈련 시키는 커스텀 변환기 만들기
- `fit()`과 `transformer()` 메소드 구현

그 후, 위도 및 경도를 이 변환기의 입력으로 사용하여 이 특성을 전처리 파이프라인에 추가
- 이러면 가장 가까운 구역의 주택 중앙값에 해당하는 특성이 모델에 추가됨

`k-Nearest Neighbors` 회귀자로 제한하기보다는 모든 회귀자를 수용하는 변환기 생성
- `MetaEstimatorMixin`을 확장하고 생성자에서 필요한 Estimator 인수 넣기
- `fit()` 메서드는 이 추정기의 복제본에서 작동해야 하며 `feat_names_in_`도 저장
- `MetaEstimatorMixin`은 추정기가 필수 매개 변수로 나열되도록 하고 `get_params()` 및 `set_params()`를 업데이트하여 추정기의 하이퍼 매개 변수를 조정
- 마지막으로 `get_feature_names_out()` 메서드를 만들기
"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.base import MetaEstimatorMixin, clone
from sklearn.utils.validation import check_array, check_is_fitted

class FeatureFromRegressor(MetaEstimatorMixin, BaseEstimator, TransformerMixin):
    def __init__(self, estimator):
        self.estimator = estimator

    def fit(self, X, y=None):
        estimator_ = clone(self.estimator)
        estimator_.fit(X, y)
        self.estimator_ = estimator_
        self.n_features_in_ = self.estimator_.n_features_in_
        if hasattr(self.estimator, "feature_names_in_"):
            self.feature_names_in_ = self.estimator.feature_names_in_
        return self  # always return self!
    
    def transform(self, X):
        check_is_fitted(self)
        predictions = self.estimator_.predict(X)
        if predictions.ndim == 1:
            predictions = predictions.reshape(-1, 1)
        return predictions

    def get_feature_names_out(self, names=None):
        check_is_fitted(self)
        n_outputs = getattr(self.estimator_, "n_outputs_", 1)
        estimator_class_name = self.estimator_.__class__.__name__
        estimator_short_name = estimator_class_name.lower().replace("_", "")
        return [f"{estimator_short_name}_prediction_{i}"
                for i in range(n_outputs)]

"""Scikit-Learn의 API를 준수하는지 확인"""

from sklearn.utils.estimator_checks import check_estimator

check_estimator(FeatureFromRegressor(KNeighborsRegressor())) #준수함

knn_reg = KNeighborsRegressor(n_neighbors=3, weights="distance")
knn_transformer = FeatureFromRegressor(knn_reg)
geo_features = housing[["latitude", "longitude"]]
knn_transformer.fit_transform(geo_features, housing_labels)

"""출력된 특성의 이름"""

knn_transformer.get_feature_names_out()

"""전처리 파이프 라인에 변환기를 추가해준다.

"""

from sklearn.base import clone

transformers = [(name, clone(transformer), columns)
                for name, transformer, columns in preprocessing.transformers]
geo_index = [name for name, _, _ in transformers].index("geo")
transformers[geo_index] = ("geo", knn_transformer, ["latitude", "longitude"])

new_geo_preprocessing = ColumnTransformer(transformers)

new_geo_pipeline = Pipeline([
    ('preprocessing', new_geo_preprocessing),
    ('svr', SVR(C=rnd_search.best_params_["svr__C"],
                gamma=rnd_search.best_params_["svr__gamma"],
                kernel=rnd_search.best_params_["svr__kernel"])),
])

new_pipe_rmses = -cross_val_score(new_geo_pipeline,
                                  housing.iloc[:5000],
                                  housing_labels.iloc[:5000],
                                  scoring="neg_root_mean_squared_error",
                                  cv=3)
pd.Series(new_pipe_rmses).describe()

"""군집화 유사성 특징이 더 우수하다.

#### 5.
RandomSearchCV(랜덤 탐색)을 사용하여 pretaration 옵션을 자동 탐색
"""

param_distribs = {
    "preprocessing__geo__estimator__n_neighbors": range(1, 30),
    "preprocessing__geo__estimator__weights": ["distance", "uniform"],
    "svr__C": reciprocal(20, 200_000),
    "svr__gamma": expon(scale=1.0),
}

new_geo_rnd_search = RandomizedSearchCV(new_geo_pipeline,
                                        param_distributions=param_distribs,
                                        n_iter=50,
                                        cv=3,
                                        scoring='neg_root_mean_squared_error',
                                        random_state=42)
new_geo_rnd_search.fit(housing.iloc[:5000], housing_labels.iloc[:5000])

new_geo_rnd_search_rmse = -new_geo_rnd_search.best_score_
new_geo_rnd_search_rmse

"""군집화 유사성이 KNN 특성보다 더 낫다는걸 알 수 있다.

#### 6.
1. `StandardScalerClone` 클래스를 처음부터 다시 구현
2. `inverse_transform()` 메서드 지원 - 실행 시 x값에 가깝게 반환하는 scaler.inverse_transform(scaler.fit_transform(X)) 추가
3. 특성 이름에 대한 지원 추가 - 입력이 DataFrame인 경우 `fit()` 메서드에서 feature_names_in_을 설정
  - 속성은 열 이름의 *NumPy* 배열이어야 함
4. 마지막으로 input_features=Facebook라는 선택사항 인수가 있는  `get_feature_names_out()` 메서드 구현
 - 인수가 전달된 경우 메서드의 길이가 n_features_in_과 일치하는지, 정의된 경우 features_names_in_과 일치해야 합, 그리고 input_features가 반환 되어야 함
 - input_features가 None인 경우 - features_names_in_ 반환 또는, 길이가 n_features_in_인 np.array(["x0", "x1", ...]) 반환
"""

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):  # no *args or **kwargs!
        self.with_mean = with_mean

    def fit(self, X, y=None):  # y is required even though we don't use it
        X_orig = X
        X = check_array(X)  # checks that X is an array with finite float values
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()
        if hasattr(X_orig, "columns"):
            self.feature_names_in_ = np.array(X_orig.columns, dtype=object) # np.object 대신 object 사용
        return self  # always return self!

    def transform(self, X):
        check_is_fitted(self)  # looks for learned attributes (with trailing _)
        X = check_array(X)
        if self.n_features_in_ != X.shape[1]:
            raise ValueError("Unexpected number of features")
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
    
    def inverse_transform(self, X):
        check_is_fitted(self)
        X = check_array(X)
        if self.n_features_in_ != X.shape[1]:
            raise ValueError("Unexpected number of features")
        X = X * self.scale_
        return X + self.mean_ if self.with_mean else X
    
    def get_feature_names_out(self, input_features=None):
        if input_features is None:
            return getattr(self, "feature_names_in_",
                           [f"x{i}" for i in range(self.n_features_in_)])
        else:
            if len(input_features) != self.n_features_in_:
                raise ValueError("Invalid number of features")
            if hasattr(self, "feature_names_in_") and not np.all(
                self.feature_names_in_ == input_features
            ):
                raise ValueError("input_features ≠ feature_names_in_")
            return input_features

"""커스텀 변환기 테스트"""

from sklearn.utils.estimator_checks import check_estimator
 
check_estimator(StandardScalerClone())
# 에러 없고, Scikit-Learn API 준수

"""예상한 대로 변환이 작동하는지 확인"""

np.random.seed(42)
X = np.random.rand(1000, 3)

scaler = StandardScalerClone()
X_scaled = scaler.fit_transform(X)

assert np.allclose(X_scaled, (X - X.mean(axis=0)) / X.std(axis=0))

# with_mean=False로 설정 했을 때
scaler = StandardScalerClone(with_mean=False)
X_scaled_uncentered = scaler.fit_transform(X)

assert np.allclose(X_scaled_uncentered, X / X.std(axis=0))

#역(반대)가 작동 하는지
scaler = StandardScalerClone()
X_back = scaler.inverse_transform(scaler.fit_transform(X))
assert np.allclose(X, X_back)

# feature names out이 제대로 작동 하는지 
assert np.all(scaler.get_feature_names_out() == ["x0", "x1", "x2"])
assert np.all(scaler.get_feature_names_out(["a", "b", "c"]) == ["a", "b", "c"])

#데이터 프레임을 fit 한 경우, 내부와 외부 특성에 문제가 없는지
df = pd.DataFrame({"a": np.random.rand(100), "b": np.random.rand(100)})
scaler = StandardScalerClone()
X_scaled = scaler.fit_transform(df)

assert np.all(scaler.feature_names_in_ == ["a", "b"])
assert np.all(scaler.get_feature_names_out() == ["a", "b"])

